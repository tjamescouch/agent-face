# agent-face

Real-time text sentiment to animated face expression pipeline, built from scratch with zero dependencies.

## Architecture

```
stdin (text lines)
  │
  ▼
┌──────────────┐   ┌──────────────┐   ┌──────────────────┐   ┌──────────┐
│  Tokenizer   │──▶│  Neural Net  │──▶│ ExpressionMapper  │──▶│ Renderer │
│  + Vocab     │   │  (6-class)   │   │  (EMA smoothing)  │   │ (plugin) │
└──────────────┘   └──────────────┘   └──────────────────┘   └──────────┘
  bag-of-words       softmax scores     smoothed scores +       NDJSON /
  encoding           per emotion        30 landmark points      ANSI / Canvas
```

## Quick Start

```sh
# Train the sentiment model
node face.js train

# Run the demo (animated ANSI face)
node face.js demo

# Stream text from stdin
echo "I am so happy today" | node face.js run --renderer json
```

## CLI Reference

```
Usage: face <command> [options]

Commands:
  train [--epochs N] [--lr F]          Train sentiment model, save weights
  run [--renderer ansi|json|canvas]    Stream stdin → face
       [--fps N] [--smoothing F]
  demo [--renderer ansi] [--delay MS]  Built-in demo text
  landmarks                            Print landmark schema as JSON
  eval                                 Model accuracy breakdown
```

## Frame Schema

Each render tick produces a frame object:

```json
{
  "timestamp": 1234,
  "sentiment": {
    "joy": 0.82,
    "anger": 0.03,
    "sadness": 0.02,
    "surprise": 0.05,
    "fear": 0.01,
    "neutral": 0.07
  },
  "dominant": "joy",
  "points": [
    { "name": "brow_L_inner", "group": "eyebrow_left", "x": 0.35, "y": 0.27 },
    ...
  ]
}
```

- `sentiment` — softmax scores per emotion, sum to 1
- `dominant` — highest-scoring emotion
- `points` — 30 face landmarks in normalized [0,1] coordinates, deformed by emotion weights

## File Structure

```
face/
├── face.js             # CLI entry + pipeline orchestrator
├── nn.js               # Matrix math, activations, feed-forward network, SGD
├── sentiment.js        # Tokenizer, Vocabulary, SentimentAnalyzer
├── training-data.js    # ~300 labeled examples across 6 emotions
├── landmarks.js        # 30 face landmarks + per-emotion deformation vectors
├── expression.js       # ExpressionMapper (EMA smoothing, frame generation)
├── renderer.js         # RendererManager (plugin loader)
├── renderers/
│   ├── json.js         # NDJSON output (for piping)
│   ├── ansi.js         # Terminal Unicode art + emotion bars
│   └── canvas.js       # Browser Canvas via HTTP + SSE
├── face.test.js        # Test suite (~80 tests)
├── weights.json        # Saved model weights (generated by train)
└── package.json
```

## Renderer Plugin Contract

A renderer is an ES module exporting an object with:

```js
export default {
  init(config) { },    // called once with { stream, ... }
  render(frame) { },   // called each tick with a frame object
  close() { },         // called on shutdown
};
```

Built-in renderers:
- **json** — NDJSON to stdout/stream, one frame per line
- **ansi** — terminal Unicode face with emotion bars
- **canvas** — starts HTTP server, streams frames via SSE to browser Canvas

## How It Works

1. **Tokenize** — input text is lowercased, punctuation stripped, split into tokens
2. **Encode** — tokens mapped to a bag-of-words vector via a learned vocabulary (top 500 words by frequency)
3. **Classify** — feed-forward neural network (input → hidden ReLU → softmax output) produces 6 emotion probabilities
4. **Smooth** — ExpressionMapper applies exponential moving average to blend between emotions over time
5. **Deform** — 30 neutral face landmarks are displaced by per-emotion deformation vectors, weighted by smoothed scores
6. **Render** — a frame (timestamp, scores, dominant emotion, deformed points) is passed to the active renderer plugin
